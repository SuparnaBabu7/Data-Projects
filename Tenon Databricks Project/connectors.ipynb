{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b64b6920-e1c5-4847-920d-d813eb13931e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def blobconnector(in_df):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonblobstorage.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"blobkey\"))\n",
    "    print(\"connection sucessful\") #setting up the blob conenctor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1944a9-24ed-4692-ac2b-c175bd0f3364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readblbdata(fn):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonblobstorage.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"blobkey\"))\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"abfss://tenonraw@tenonblobstorage.dfs.core.windows.net/\"+fn)\n",
    "    print(df)\n",
    "    return df #setting up the read blob data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4513a7c-6d16-4f3b-9c61-7cac10a9e551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def adlsconnector(in_df):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonadls.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"adlskey\"))\n",
    "    print(\"adls connection sucessful\") #setting up the adls conenctor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a3f48a2-3398-43ba-bb08-fb9a65d20f05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readbronzedata(fn):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonadls.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"adlskey\"))\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"abfss://tenon@tenonadls.dfs.core.windows.net/bronze/\"+fn)\n",
    "    return df\n",
    "    #setting up the read bronze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "195fe9c7-05be-48bf-9c35-83769dfd54ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readbronzedatajson(fn):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonadls.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"adlskey\"))\n",
    "    df = spark.read.format(\"json\").load(\"abfss://tenon@tenonadls.dfs.core.windows.net/bronze/\"+fn)\n",
    "    return df\n",
    "    #setting up the read bronze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87e092b6-019a-443d-8a48-11e480d93351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readsilverdata(fn):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonadls.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"adlskey\"))\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"abfss://tenon@tenonadls.dfs.core.windows.net/silver/\"+fn)\n",
    "    return df\n",
    "    #setting up the read silver data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba2b040-9f1b-4ae7-90a5-0a3183cc1201",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write2silver(df,file_name):\n",
    "    silver_path = \"abfss://tenon@tenonadls.dfs.core.windows.net/silver/\"\n",
    "    temp_path = f\"{silver_path}/output_temp\"\n",
    "    final_path = f\"{silver_path}/{file_name}\"\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(temp_path)\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    csv_file = [f.path for f in files if f.path.endswith(\".csv\")][0]\n",
    "    dbutils.fs.mv(csv_file,final_path)\n",
    "    dbutils.fs.rm(temp_path,recurse=True)\n",
    "    print(f\"CSV file wrtitten to {final_path}\")\n",
    "    return final_path    \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67e5a778-10b7-42ae-b304-c8b285e76565",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write2gold(df,file_name):\n",
    "    gold_path = \"abfss://tenon@tenonadls.dfs.core.windows.net/gold/\"\n",
    "    temp_path = f\"{gold_path}/output_temp\"\n",
    "    final_path = f\"{gold_path}/{file_name}\"\n",
    "    df.coalesce(1).write.mode(\"overwrite\").option(\"header\",\"true\").csv(temp_path)\n",
    "    files = dbutils.fs.ls(temp_path)\n",
    "    csv_file = [f.path for f in files if f.path.endswith(\".csv\")][0]\n",
    "    dbutils.fs.mv(csv_file,final_path)\n",
    "    dbutils.fs.rm(temp_path,recurse=True)\n",
    "    print(f\"CSV file wrtitten to {final_path}\")\n",
    "    return final_path    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36247b90-9d1d-4154-8578-70b0beca1298",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readgolddata(fn):\n",
    "    spark.conf.set(\"fs.azure.account.key.tenonadls.dfs.core.windows.net\",dbutils.secrets.get(scope=\"tenonscope\", key=\"adlskey\"))\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"abfss://tenon@tenonadls.dfs.core.windows.net/gold/\"+fn)\n",
    "    return df\n",
    "    #setting up the read gold data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935504c4-2ca5-4c29-8015-82eeeeab0a1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_to_azure_sql(df, table_name, server_name, database_name, username, password):\n",
    "    jdbc_url = f\"jdbc:sqlserver://{server_name};database={database_name}\"\n",
    "    connection_properties = {\n",
    "        \"user\": username,\n",
    "        \"password\": password,\n",
    "        \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "    }\n",
    "    df.write.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=table_name,\n",
    "        mode=\"overwrite\",\n",
    "        properties=connection_properties\n",
    "    )\n",
    "\n",
    "# Example usage:\n",
    "# write_to_azure_sql(pati_hospi_claim_subscriber_subgrp_disese_group_df, \"your_table_name\", \"your_server_name\", \"your_database_name\", \"your_username\", \"your_password\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "connectors",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}